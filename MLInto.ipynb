{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e564b7ac-11f1-4952-9d81-06c0c387c475",
   "metadata": {},
   "source": [
    "# An Adventure in Algorithms: Dipping Your Toes into Machine Learning\n",
    "\n",
    "Many of the following demonstrations are from the [scikit-learn tutorials](https://scikit-learn.org/stable/index.html):  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c0881c-75f0-45c0-958b-be972332457f",
   "metadata": {},
   "source": [
    "## A Simple Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e0394a-ae02-49d9-bc4b-199b457bb09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import datasets, linear_model, neural_network, metrics, svm, neighbors, tree\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split, LearningCurveDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea23c0b2-e3ac-4c24-a35d-36c680cf5edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 9485\n",
    "\n",
    "random.seed(seed)\n",
    "\n",
    "m_orig = random.uniform(0.5, 3)\n",
    "b_orig = random.uniform(-2,5)\n",
    "N = 8\n",
    "xmin = 0.5\n",
    "xmax = 3\n",
    "\n",
    "x = np.array([round(random.uniform(xmin, xmax), 2)for _ in range(N)])\n",
    "y = np.array([round(m_orig*xi + b_orig + random.normalvariate(mu=0.0, sigma=.35),2) for xi in x])\n",
    "\n",
    "plt.scatter(x, y, color='blue')\n",
    "plt.xlim(0, 3)\n",
    "plt.ylim(-2, 14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac432c15-c66b-48b8-8ce8-d91c31543a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_new_line(ms, bs, x, y):\n",
    "    xs = np.array([0,3])\n",
    "\n",
    "    # 2 Plots, side-by-side\n",
    "    fig, (ax1, ax2) = plt.subplots(1,2, figsize=(10,4), sharey=True)\n",
    "\n",
    "    # Add the most recent line\n",
    "    ax1.plot(xs, ms[-1]*xs+bs[-1], color='red')\n",
    "\n",
    "    # Add lines for the previous attempts\n",
    "    for (i,(m, b)) in enumerate(zip(ms, bs)):\n",
    "        color = 'red' if i == len(ms)-1 else 'gray'\n",
    "        ax2.plot(xs, m*xs+b, color=color)\n",
    "\n",
    "    # Draw the scatter plot on top of the lines, prevents the previous attempts from cluttering up the data\n",
    "    ax1.scatter(x, y, color='blue')\n",
    "    ax2.scatter(x, y, color='blue')\n",
    "    plt.xlim(0, 3)\n",
    "    plt.ylim(-2, 14)\n",
    "    plt.show()\n",
    "\n",
    "    return [mean_squared_error(y, m*x+b) for (m,b) in zip(ms, bs)]\n",
    "\n",
    "ms=[]\n",
    "bs=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bcff94-207c-4b22-b402-53d08338fd15",
   "metadata": {},
   "source": [
    "Remember that a line is defined as $y = mx + b$, so we need to specify values for $m$ and $b$. These are called parameters of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b29adf0-6ac2-4993-bb52-88cb6b798351",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 0.0\n",
    "b = 0.0\n",
    "\n",
    "ms.append(m)\n",
    "bs.append(b)\n",
    "mses = plot_new_line(ms, bs, x, y)\n",
    "print('m         b         mse')\n",
    "for m, b, mse in zip(ms, bs, mses):\n",
    "    print('{m:.4f}    {b:.4f}    {mse:.5f}'.format(m=m, b=b, mse=mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1348ed92-a75a-45b8-a577-cd02345d99cd",
   "metadata": {},
   "source": [
    "## Let's Do This Automically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbcc4f5-afae-4301-913f-b585835dcc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create linear regression object\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "# Train the model using the training sets\n",
    "regr.fit(x.reshape(-1,1), y.reshape(-1,1))\n",
    "\n",
    "# Plot the data\n",
    "plt.scatter(x,y, color='blue')\n",
    "\n",
    "# Plot the line which minimizes MSE\n",
    "x_toplot = np.array([0,3]).reshape(-1,1)\n",
    "y_toplot = regr.predict(x_toplot)\n",
    "plt.plot(x_toplot, y_toplot, color='red')\n",
    "plt.xlim(0, 3)\n",
    "plt.ylim(-2, 14);\n",
    "\n",
    "# Print the MSE and the parameters for the line\n",
    "#print(f'MSE: {mean_squared_error(y, regr.predict(x.reshape(-1,1)))}')\n",
    "#print(f'm = {regr.coef_[0][0]} \\nb = {regr.intercept_[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9474c111-b12a-4842-bafe-66d58b435095",
   "metadata": {},
   "source": [
    "## Back to Presentation for a Discussion About Training and Test Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35006fe-3e2f-4dad-ba96-28265726b18d",
   "metadata": {},
   "source": [
    "## A More Practical Example\n",
    "\n",
    "The following code loads a dataset that includes a \"toy\" dataset which is taken from a diabetes dataset. We are narrowing to one \"feature\" here (normalized BMI) and predicting a \"quantitative measure of disease progression one year after baseline.\" The data comes from [this site](https://www4.stat.ncsu.edu/~boos/var.select/diabetes.html).\n",
    "\n",
    "In the following plot, the black dots are the training set, while the red dots are the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4660589d-d35a-48eb-9423-da8563902ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the diabetes dataset\n",
    "diabetes_X, diabetes_y = datasets.load_diabetes(return_X_y=True)\n",
    "\n",
    "# Use only one feature (BMI, normalized)\n",
    "diabetes_X = diabetes_X[:, np.newaxis, 2]\n",
    "\n",
    "# Print the number of data points\n",
    "print(f'Number of data points: {diabetes_X.shape[0]}')\n",
    "\n",
    "# Split the data into training/testing sets\n",
    "diabetes_X_train = diabetes_X[:-88]\n",
    "diabetes_X_test = diabetes_X[-88:]\n",
    "\n",
    "# Split the targets into training/testing sets\n",
    "diabetes_y_train = diabetes_y[:-88]\n",
    "diabetes_y_test = diabetes_y[-88:]\n",
    "\n",
    "# Print information about the training and testing sets\n",
    "print(f'Number of data points in the training set: {diabetes_X_train.shape[0]}')\n",
    "print(f'Number of data points in the test set: {diabetes_X_test.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69e4fda-9ce6-4e55-8e59-45fee99fad0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data (training set in black, test set in red)\n",
    "plt.scatter(diabetes_X_train, diabetes_y_train, color=\"black\")\n",
    "plt.scatter(diabetes_X_test, diabetes_y_test, color=\"red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0abf40-c2fa-40d5-920a-dab6a3b56dfb",
   "metadata": {},
   "source": [
    "Now, let's set up a linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2666e20-8665-46b1-ab9c-f0d3cb0740ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create linear regression object\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "# Train the model using the training sets\n",
    "regr.fit(diabetes_X_train, diabetes_y_train)\n",
    "\n",
    "# Make predictions using the testing set\n",
    "diabetes_y_pred = regr.predict(diabetes_X_test)\n",
    "\n",
    "# The coefficients\n",
    "print(f'MSE: {mean_squared_error(diabetes_y_test, diabetes_y_pred)}')\n",
    "print(f'm = {regr.coef_[0]} \\nb = {regr.intercept_}')\n",
    "\n",
    "# For plotting the line of best fit, get the min and max x value so that we can build the line to match all the data\n",
    "xmin = min(diabetes_X_train.min(), diabetes_X_test.min())\n",
    "xmax = max(diabetes_X_train.max(), diabetes_X_test.max())\n",
    "x_plot = np.array([xmin, xmax]).reshape(-1,1)\n",
    "y_plot = regr.predict(x_plot)\n",
    "\n",
    "# Plot outputs\n",
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(10, 4), sharey=True)\n",
    "ax1.scatter(diabetes_X_train, diabetes_y_train, color=\"black\")\n",
    "ax2.scatter(diabetes_X_test, diabetes_y_test, color=\"red\")\n",
    "ax1.plot(x_plot, y_plot, color=\"blue\", linewidth=3)\n",
    "ax2.plot(x_plot, y_plot, color=\"blue\", linewidth=3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01d6379-4812-41a2-83d6-dca73714e2c9",
   "metadata": {},
   "source": [
    "## Let's Talk About Neural Networks\n",
    "\n",
    "1. Watch the 3blue1brown video\n",
    "2. Play with the [tensorflow playground](http://playground.tensorflow.org)\n",
    "3. Continue below where we create our own neural network to recognize digits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cce840-1f1f-4b83-a57b-e31195f2ed0c",
   "metadata": {},
   "source": [
    "## A Neural Network for Recognizing Handwritten Digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87d1d0a-b802-4cbe-89c3-d187225d886e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST dataset\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "# Plot a few of the digits\n",
    "_, axes = plt.subplots(nrows=2, ncols=8, figsize=(15, 5))\n",
    "for ax, image, label in zip(axes.flatten(), digits.images, digits.target):\n",
    "    ax.set_axis_off()\n",
    "    ax.imshow(image, cmap=plt.cm.gray_r, interpolation=\"nearest\")\n",
    "    ax.set_title(\"Training: %i\" % label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dba69a-48b1-41f7-8042-fa0aebad2be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten the images\n",
    "n_samples = len(digits.images)\n",
    "data = digits.images.reshape((n_samples, -1))/16\n",
    "\n",
    "# Let's see the images as lists of numbers\n",
    "print(f'data is {data.shape[0]} by {data.shape[1]}, in other words, we have {data.shape[0]} images, flattened down so that each has {data.shape[1]} numbers associated with it')\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a801cef1-ca2d-4f22-a5b5-0d8f1fe72270",
   "metadata": {},
   "source": [
    "#### Notice that the above are only 8x8 images with a total of 64 pixels, we are using the smaller size here so that the computation is faster and we're not waiting around for the computer to do the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b76844f-6da1-4461-8d0d-37f687b1e959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a training and testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data, digits.target, test_size=0.2, shuffle=True\n",
    ")\n",
    "\n",
    "# Print the number of training and testing samples\n",
    "print(f'There are {X_train.shape[0]} samples in the training set.')\n",
    "print(f'There are {X_test.shape[0]} samples in the test set.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f1d8be-a626-433f-b1e0-5a250ef4813a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up some hyperparameters first - these control the structure of the neural network and how we train it\n",
    "hidden_layer_sizes = (50)\n",
    "activation = 'logistic' # Other options are 'relu', 'tanh', 'identity'\n",
    "learning_rate = 0.01\n",
    "max_iter = 700\n",
    "\n",
    "# Create the Neural Network Classifier Object\n",
    "mlp = neural_network.MLPClassifier(hidden_layer_sizes = hidden_layer_sizes, \n",
    "                                   activation = activation, \n",
    "                                   learning_rate_init = learning_rate, \n",
    "                                   max_iter = max_iter)\n",
    "\n",
    "# Train the network\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "# Now predict the labels for the test set\n",
    "predicted = mlp.predict(X_test)\n",
    "\n",
    "# Let's visualize some predictions\n",
    "_, axes = plt.subplots(nrows=2, ncols=8, figsize=(15, 5))\n",
    "for ax, image, prediction in zip(axes.flatten(), X_test, predicted):\n",
    "    ax.set_axis_off()\n",
    "    image = image.reshape(8, 8)\n",
    "    ax.imshow(image, cmap=plt.cm.gray_r, interpolation=\"nearest\")\n",
    "    ax.set_title(f\"Prediction: {prediction}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285d8296-98e4-43a0-8447-613b660072af",
   "metadata": {},
   "source": [
    "## Some Analysis of the Results\n",
    "\n",
    "First, we'll start with some statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9de9fd4-ebd3-48db-8f18-f607d11c5cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_corr_preds_test = sum(y_test == predicted)\n",
    "n_corr_preds_train = sum(y_train == mlp.predict(X_train))\n",
    "print('Training set: ')\n",
    "print(f'  {n_corr_preds_train} correct predictions')\n",
    "print(f'  {y_train.shape[0] - n_corr_preds_train} incorrect predictions')\n",
    "print(f'  Accuracy: {n_corr_preds_train*100/y_train.shape[0]}% correct')\n",
    "print('Test set: ')\n",
    "print(f'  {n_corr_preds_test} correct predictions')\n",
    "print(f'  {y_test.shape[0] - n_corr_preds_test} incorrect predictions')\n",
    "print(f'  Accuracy: {n_corr_preds_test*100/y_test.shape[0]}% correct')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d2e717-d133-4dab-afed-19f4c6794a41",
   "metadata": {},
   "source": [
    "#### We can also look at the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76effe81-9775-4b2a-9f23-c2a33043b373",
   "metadata": {},
   "outputs": [],
   "source": [
    "disp = metrics.ConfusionMatrixDisplay.from_predictions(y_test, predicted)\n",
    "disp.figure_.suptitle(\"Confusion Matrix for Test Set\")\n",
    "# print(f\"Confusion matrix:\\n{disp.confusion_matrix}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d828fef9-59b1-4808-a2b1-6dc5a465b70e",
   "metadata": {},
   "source": [
    "### Let's look at the samples that got mis-labeled:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33b838b-a71d-4a02-a6d8-5ccd733f7dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the mislabeled images\n",
    "mislabeled = (y_test != predicted)\n",
    "\n",
    "# Plot them\n",
    "n_rows = math.ceil(sum(mislabeled) / 8)\n",
    "_, axes = plt.subplots(nrows=n_rows, ncols=8, figsize=(15, 5))\n",
    "for ax, image, prediction, true_label in zip(axes.flatten(), X_test[mislabeled], predicted[mislabeled], y_test[mislabeled]):\n",
    "    ax.set_axis_off()\n",
    "    image = image.reshape(8, 8)\n",
    "    ax.imshow(image, cmap=plt.cm.gray_r, interpolation=\"nearest\")\n",
    "    ax.set_title(f\"Pred: {prediction}; True: {true_label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e562495-fc8a-42e8-a07d-17a2505f58f8",
   "metadata": {},
   "source": [
    "### Take another look at the training loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49194783-69d7-4109-9848-621290606248",
   "metadata": {},
   "source": [
    "## Can Other Models Do Any Better?\n",
    "\n",
    "### First, The Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98438fea-2cd1-4dc0-ab16-81bd8d610891",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.001\n",
    "\n",
    "clf = svm.SVC(gamma=gamma)\n",
    "clf.fit(X_train, y_train)\n",
    "predicted = clf.predict(X_test)\n",
    "\n",
    "n_corr_preds_test = sum(y_test == predicted)\n",
    "n_corr_preds_train = sum(y_train == mlp.predict(X_train))\n",
    "print('Training set: ')\n",
    "print(f'  {n_corr_preds_train} correct predictions')\n",
    "print(f'  {y_train.shape[0] - n_corr_preds_train} incorrect predictions')\n",
    "print(f'  Accuracy: {n_corr_preds_train*100/y_train.shape[0]}% correct')\n",
    "print('Test set: ')\n",
    "print(f'  {n_corr_preds_test} correct predictions')\n",
    "print(f'  {y_test.shape[0] - n_corr_preds_test} incorrect predictions')\n",
    "print(f'  Accuracy: {n_corr_preds_test*100/y_test.shape[0]}% correct')\n",
    "\n",
    "disp = metrics.ConfusionMatrixDisplay.from_predictions(y_test, predicted)\n",
    "disp.figure_.suptitle(\"Confusion Matrix for Test Set\")\n",
    "# print(f\"Confusion matrix:\\n{disp.confusion_matrix}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78379d09-ceb9-4359-b029-72e5413fd420",
   "metadata": {},
   "source": [
    "### How About K-Nearest Neighbors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d1bb8a-a5fd-416e-a392-743c2bce4642",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors = 3\n",
    "\n",
    "clf = neighbors.KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "clf.fit(X_train, y_train)\n",
    "predicted = clf.predict(X_test)\n",
    "\n",
    "n_corr_preds_test = sum(y_test == predicted)\n",
    "n_corr_preds_train = sum(y_train == mlp.predict(X_train))\n",
    "print('Training set: ')\n",
    "print(f'  {n_corr_preds_train} correct predictions')\n",
    "print(f'  {y_train.shape[0] - n_corr_preds_train} incorrect predictions')\n",
    "print(f'  Accuracy: {n_corr_preds_train*100/y_train.shape[0]}% correct')\n",
    "print('Test set: ')\n",
    "print(f'  {n_corr_preds_test} correct predictions')\n",
    "print(f'  {y_test.shape[0] - n_corr_preds_test} incorrect predictions')\n",
    "print(f'  Accuracy: {n_corr_preds_test*100/y_test.shape[0]}% correct')\n",
    "\n",
    "disp = metrics.ConfusionMatrixDisplay.from_predictions(y_test, predicted)\n",
    "disp.figure_.suptitle(\"Confusion Matrix for Test Set\")\n",
    "# print(f\"Confusion matrix:\\n{disp.confusion_matrix}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead5f192-06d4-4840-8245-e39086f926d9",
   "metadata": {},
   "source": [
    "### What About Decision Trees?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5695d09-cc47-44af-9ab5-54dc3fed672a",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = tree.DecisionTreeClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "predicted = clf.predict(X_test)\n",
    "\n",
    "n_corr_preds_test = sum(y_test == predicted)\n",
    "n_corr_preds_train = sum(y_train == mlp.predict(X_train))\n",
    "print('Training set: ')\n",
    "print(f'  {n_corr_preds_train} correct predictions')\n",
    "print(f'  {y_train.shape[0] - n_corr_preds_train} incorrect predictions')\n",
    "print(f'  Accuracy: {n_corr_preds_train*100/y_train.shape[0]}% correct')\n",
    "print('Test set: ')\n",
    "print(f'  {n_corr_preds_test} correct predictions')\n",
    "print(f'  {y_test.shape[0] - n_corr_preds_test} incorrect predictions')\n",
    "print(f'  Accuracy: {n_corr_preds_test*100/y_test.shape[0]}% correct')\n",
    "\n",
    "disp = metrics.ConfusionMatrixDisplay.from_predictions(y_test, predicted)\n",
    "disp.figure_.suptitle(\"Confusion Matrix for Test Set\")\n",
    "# print(f\"Confusion matrix:\\n{disp.confusion_matrix}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fbf23e-6d18-4bdb-8bcd-01301e62b3fb",
   "metadata": {},
   "source": [
    "# Unsupervised Learning\n",
    "\n",
    "First a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3756222-c00d-4878-a161-2e004f04be5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "X, y_true = make_blobs(n_samples=300, centers=4,\n",
    "                       cluster_std=0.60, random_state=0)\n",
    "plt.scatter(X[:, 0], X[:, 1], s=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6ec662-ecf3-4203-af6d-0d638dbac2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=4)\n",
    "kmeans.fit(X)\n",
    "y_kmeans = kmeans.predict(X)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis')\n",
    "\n",
    "centers = kmeans.cluster_centers_\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a49e14-cd14-4e1e-8339-59bcff976969",
   "metadata": {},
   "source": [
    "## How about a more practical example?\n",
    "\n",
    "When images are stored on a computer, the standard \"lossless\" format requires being able to store 96,615 possible colors for each pixel. This creates a large file size, but maintains as much detail as possible.\n",
    "\n",
    "It's often convenient to try and compress images. One of the popular formats is the Graphics Interchange Format (GIF). In this format, a pallete of 64 colors is chosen and all the pixels in the image are re-mapped to one of the 64 possible colors. Choosing those colors can be a complicated process, but one way to do it is to use a clustering algorithm (so that similar colors are in the same cluster). We implement this below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9aba0c9-07e0-4f29-bbca-fb8d30e88bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import load_sample_image\n",
    "from sklearn.metrics import pairwise_distances_argmin\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "n_colors = 64\n",
    "\n",
    "# Load a sample image\n",
    "pic = load_sample_image(\"china.jpg\")\n",
    "\n",
    "# Convert to floats instead of the default 8 bits integer coding. Dividing by\n",
    "# 255 is important so that plt.imshow works well on float data (need to\n",
    "# be in the range [0-1])\n",
    "pic = np.array(pic, dtype=np.float64) / 255\n",
    "\n",
    "# Load Image and transform to a 2D numpy array.\n",
    "w, h, d = original_shape = tuple(pic.shape)\n",
    "assert d == 3\n",
    "image_array = np.reshape(pic, (w * h, d))\n",
    "\n",
    "image_array_sample = shuffle(image_array, random_state=0, n_samples=1_500)\n",
    "kmeans = KMeans(n_clusters=n_colors, random_state=0).fit(image_array_sample)\n",
    "\n",
    "# Get labels for all points\n",
    "labels = kmeans.predict(image_array)\n",
    "\n",
    "def recreate_image(codebook, labels, w, h):\n",
    "    \"\"\"Recreate the (compressed) image from the code book & labels\"\"\"\n",
    "    return codebook[labels].reshape(w, h, -1)\n",
    "\n",
    "# Display all results, alongside original image\n",
    "plt.figure(1)\n",
    "plt.clf()\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Original image (96,615 colors)\")\n",
    "plt.imshow(pic)\n",
    "\n",
    "plt.figure(2)\n",
    "plt.clf()\n",
    "plt.axis(\"off\")\n",
    "plt.title(f\"Quantized image ({n_colors} colors, K-Means)\")\n",
    "plt.imshow(recreate_image(kmeans.cluster_centers_, labels, w, h));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eaed44e-89c5-4aad-88c9-9e7bafe31b6a",
   "metadata": {},
   "source": [
    "#### We can even show the color palette:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dba7c6f-c758-4efd-bd58-7a64c7307374",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(13,7))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(kmeans.cluster_centers_.reshape(1, n_colors, -1));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f27632-a2eb-4ed4-875a-66d6b2ac69e0",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "\n",
    "Check out [this video](https://youtu.be/kopoLzvh5jY?feature=shared)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
